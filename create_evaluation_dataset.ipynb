{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "c4202b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "from numpy import loadtxt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "a6c6336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "fe515a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return bounding box of prometeus input\n",
    "def get_bbox(df):\n",
    "    list = []\n",
    "    \n",
    "    list.append(df[\"XLAT\"].min().values)\n",
    "    list.append(df[\"XLAT\"].max().values)\n",
    "    list.append(df[\"XLONG\"].min().values)\n",
    "    list.append(df[\"XLONG\"].max().values)\n",
    "    \n",
    "    return list\n",
    "\n",
    "#Return remapped dataset to specific spatial resolution\n",
    "def remap(df, bbox):\n",
    "    global gridsize\n",
    "        \n",
    "    ds_out = xe.util.grid_2d(bbox[2], bbox[3], gridsize, bbox[0], bbox[1], gridsize)        \n",
    "    \n",
    "    #Define a new grid with bbox and gridsize \n",
    "    #Determining time variable name\n",
    "    if \"time\" in list(df.dims):\n",
    "        regridder = xe.Regridder(df.isel(time=0), ds_out, \"bilinear\")\n",
    "    else:\n",
    "        regridder = xe.Regridder(df.isel(Time=0), ds_out, \"bilinear\")\n",
    "    \n",
    "    #Remapping input data with new grid specifications\n",
    "    out = regridder(df)\n",
    "    \n",
    "    return out\n",
    "\n",
    "#Return nearest point from dataset in x,y format \n",
    "def get_nearest_point(ds, lat, lon): \n",
    "    abslat = np.abs(ds.lat-lat)\n",
    "    abslon = np.abs(ds.lon-lon)\n",
    "    \n",
    "    c = np.maximum(abslon, abslat)    \n",
    "    \n",
    "    ([xloc], [yloc]) = np.where(c==np.min(c))\n",
    "        \n",
    "    return xloc, yloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "5f4b5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read nc files, extract, transform and return resampled dataset\n",
    "#Works with any wrfout files\n",
    "def get_wrf_df(date, source, drop, bpath, rpath, bname):    \n",
    "    global lat, lon, bbox\n",
    "    \n",
    "    wrf_date = pd.to_datetime(date).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    \n",
    "    #Define full path of file\n",
    "    full_path = bpath + rpath + date + bname + wrf_date\n",
    "\n",
    "    #Open dataset with xarray\n",
    "    df = xr.open_dataset(full_path)\n",
    "\n",
    "    #Drop unused variables\n",
    "    df = df.drop(drop)\n",
    "    \n",
    "    #Get bbox of prometeus lat_min, lat_max, lon_min, lon_max\n",
    "#     bbox = get_bbox(df)    \n",
    "    \n",
    "    #Modify Times format, from byte to datetime\n",
    "    time_strs = [str(i.values)[1:].replace(\"_\",\" \") for i in df[\"Times\"]]\n",
    "    time_datetime = pd.to_datetime(time_strs)\n",
    "    \n",
    "    #Modify Time dimension to time (standar name in lowercase)\n",
    "    ds_wrf_timedim = df.rename({'Time':'time'})\n",
    "    \n",
    "    #Asign new time format strings \n",
    "    ds_wrf_timecoord = ds_wrf_timedim.assign(time=time_datetime)\n",
    "    \n",
    "    #Drop Times varible\n",
    "    df = ds_wrf_timecoord.drop('Times')\n",
    "\n",
    "    #Select only 24 hours from input\n",
    "    df = df.sel(time=slice(date))\n",
    "    \n",
    "    #Remap to new gridisze    \n",
    "    remapped_df = remap(df, bbox)    \n",
    "    \n",
    "    #Eliminar los index y asignar time como nuevo, y convertirlo a datetime\n",
    "    df_out = remapped_df.to_dataframe().reset_index()\n",
    "    \n",
    "    #Rename column time to date \n",
    "    df_out = df_out.rename({'time':'date'}, axis='columns')\n",
    "    \n",
    "    df_out[\"RAIN\"] = df_out[\"RAINC\"] + df_out[\"RAINNC\"]\n",
    "    df_out[source] = df_out.groupby(['x', 'y'])['RAIN'].diff().fillna(df_out[\"RAIN\"][0])\n",
    "    df_out[source] = df_out[source].round(3)\n",
    "    df_out.drop(columns=[\"RAIN\", \"RAINC\", \"RAINNC\"], inplace=True)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "#Read GPM files, extract, transform and return resampled dataset\n",
    "#Works with any Global Precipitation Measurement dataset\n",
    "def get_gpm_df(date, drop, bpath, rpath):\n",
    "    global lat, lon, bbox\n",
    "    \n",
    "    wrf_date = pd.to_datetime(date).strftime(\"%Y-%m-%d_%H:%M:%S\") \n",
    "\n",
    "    #Define full path of GPM files\n",
    "    full_path = bpath + rpath\n",
    "\n",
    "    #Merge al GPM files of current date\n",
    "    df = xr.merge([xr.open_dataset(f) for f in glob.glob(f'{full_path}/*{date}*')])\n",
    "    \n",
    "    #Drop unused variables\n",
    "    df = df.drop(drop)\n",
    "    \n",
    "    #Remap to new gridisze    \n",
    "    remapped_df = remap(df, bbox)  \n",
    "    \n",
    "    #Eliminar los index y asignar time como nuevo, y convertirlo a datetime\n",
    "    df_out = remapped_df.to_dataframe().reset_index()\n",
    "    \n",
    "    df_out['time'] = pd.to_datetime(df_out['time'])\n",
    "    df_out.set_index(\"time\", inplace=True)\n",
    "    \n",
    "    #Resample data\n",
    "    df_resampled = df_out.groupby([\"x\", \"y\", pd.Grouper(freq='h')]).agg({'precipitationCal': 'sum', 'lat':'last', 'lon':'last'})        \n",
    "    df_resampled\n",
    "    \n",
    "    #Rename columns\n",
    "    df_resampled.rename(columns={\"precipitationCal\": \"gpm\"}, inplace=True)        \n",
    "    df_resampled.rename(columns={\"time\": \"date\"}, inplace=True)\n",
    "    df_resampled[\"gpm\"] = df_resampled[\"gpm\"].round(3)            \n",
    "\n",
    "    df_resampled = df_resampled.reset_index()\n",
    "    df_resampled = df_resampled.rename({'time':'date'}, axis='columns')\n",
    "\n",
    "    return df_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "3653324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bounding Box of prometeus wrf output at 1.8km resolution\n",
    "#For a different wrfout use get_bbox function defined before\n",
    "bbox = [26.93850708, 30.97611237, -112.75549316, -109.09912109]\n",
    "\n",
    "#Gridsize in degrees, aprox 3Km\n",
    "gridsize = 0.027\n",
    "\n",
    "#Base path\n",
    "bpath = \"/LUSTRE/home/daniel/\"\n",
    "\n",
    "#Conagua wrfout path\n",
    "con_rpath = \"namelist_conagua/salidas/\"\n",
    "#Conagua output file name\n",
    "con_bname = \"/wrfout_d01_\"\n",
    "\n",
    "#Prometeus wrfout path\n",
    "pro_rpath = \"namelist_prometeus/salidas/\"\n",
    "#Prometeus output file name\n",
    "pro_bname = \"/wrfout_d02_\"\n",
    "\n",
    "#GPM files path\n",
    "gpm_rpath = \"namelist_prometeus/gpm_imerg/\"\n",
    "\n",
    "#Drop unused variables\n",
    "drop_conagua = [\"T\", \"T2\"]\n",
    "drop_prometeus = [\"T2\"]\n",
    "drop_gpm = [\"precipitationUncal\", \"randomError\", \"HQprecipitation\", \"HQprecipSource\", \n",
    "            \"IRprecipitation\", \"HQobservationTime\", \"IRkalmanFilterWeight\", \n",
    "            \"probabilityLiquidPrecipitation\", \"precipitationQualityIndex\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c9b28b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [09:19<00:00, 20.71s/it]\n"
     ]
    }
   ],
   "source": [
    "#fechas is a file that contains dates of storm events on northwest of México\n",
    "dates_file = loadtxt(\"fechas\", dtype=\"str\")\n",
    "\n",
    "#Define empty dataframes\n",
    "gpm = pd.DataFrame()\n",
    "conagua = pd.DataFrame()\n",
    "prometeus = pd.DataFrame()\n",
    "\n",
    "#Get datasets from conagua, prometus and gpm, then append to specific dataframe\n",
    "for date in tqdm(dates_file):\n",
    "    ngpm = get_gpm_df(str(date), drop_gpm, bpath, gpm_rpath)\n",
    "    gpm = gpm.append(ngpm, ignore_index=True)\n",
    "\n",
    "    nconagua = get_wrf_df(str(date), \"conagua\", drop_conagua, bpath, con_rpath, con_bname)\n",
    "    conagua = conagua.append(nconagua, ignore_index=True)\n",
    "    \n",
    "    nprometeus = get_wrf_df(str(date), \"prometeus\", drop_prometeus, bpath, pro_rpath, pro_bname)\n",
    "    prometeus = prometeus.append(nprometeus, ignore_index=True)\n",
    "    \n",
    "#Merge all dataframes into result dataframe\n",
    "# result = pd.merge(conagua, prometeus, how='left', on=['date', 'lat', 'lon'])\n",
    "# result = pd.merge(result, gpm, how='left', on=['date', 'lat', 'lon'])\n",
    "\n",
    "#Save it to merged_info.csv file\n",
    "# result.to_csv(\"merged_info.csv\", index=False)\n",
    "gpm.to_csv(\"gpm_all_points.csv\", index=False)\n",
    "prometeus.to_csv(\"prometeus_all_points.csv\", index=False)\n",
    "conagua.to_csv(\"conagua_all_points.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "bb52354a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    13219200.000\n",
       "mean            0.551\n",
       "std             1.773\n",
       "min             0.000\n",
       "25%             0.000\n",
       "50%             0.000\n",
       "75%             0.282\n",
       "max            89.777\n",
       "Name: conagua, dtype: object"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conagua[\"conagua\"].describe().apply(\"{0:.3f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "7aff4d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    13219200.000\n",
       "mean            0.579\n",
       "std             3.109\n",
       "min             0.000\n",
       "25%             0.000\n",
       "50%             0.000\n",
       "75%             0.000\n",
       "max           158.706\n",
       "Name: prometeus, dtype: object"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prometeus[\"prometeus\"].describe().apply(\"{0:.3f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "9193aebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    13219200.000\n",
       "mean            0.654\n",
       "std             2.442\n",
       "min             0.000\n",
       "25%             0.000\n",
       "50%             0.000\n",
       "75%             0.128\n",
       "max           146.194\n",
       "Name: gpm, dtype: object"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpm[\"gpm\"].describe().apply(\"{0:.3f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291b486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "tesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
